{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_rows = 999\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Scraped Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "michael_df = pd.read_csv('./Data/michael.csv')\n",
    "florence_df = pd.read_csv('./Data/florence.csv')\n",
    "florence_df = florence_df[florence_df.username != 'SouthDakotaDOT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Hyperlinks, Image Links, Certian Special Characters, and adding Date/time Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets (df):\n",
    "    df['text'] = df['text'].str.replace(r'pic.twitter.com.*[\\r\\n]*', '', regex=True)\n",
    "    df['text'] = df['text'].map(lambda x: re.sub('http[s]?:\\/\\/[^\\s]*',' ', x))\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    # Converting column to datetime, adding time column, making data column\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['time'] = df['date'].dt.time\n",
    "    df['date'] = df['date'].dt.date\n",
    "    #removing uneeded columns\n",
    "    df.drop(columns=['hashtags','type','geo'], inplace=True)\n",
    "    # removing certain special characters\n",
    "    df['text'] = df['text'].str.replace('.', '')\n",
    "    df['text'] = df['text'].str.replace('/', '')\n",
    "    df['text'] = df['text'].str.replace(',', '')\n",
    "    df['text'] = df['text'].str.replace(\"'\", '')\n",
    "    df['text'] = df['text'].str.replace(\";\", '')\n",
    "    df['text'] = df['text'].str.replace(\"(\", '')\n",
    "    df['text'] = df['text'].str.replace(\")\", '')\n",
    "    return\n",
    "\n",
    "    \n",
    "\n",
    "clean_tweets(florence_df)\n",
    "clean_tweets(michael_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding States\n",
    "---\n",
    "Adding the state names will be essential when mapping out the locations of road closures or blockages. For Hurricane Michael and Harvey, we only observed one state, Florida and Texas, respectively. Hurrican Florence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "michael_df['state'] = michael_df['username'].map(lambda x: 'Georgia' if x in ['AlbanyGaPD','511Georgia','CityofAlbanyGA'] else 'Florida')\n",
    "michael_df = michael_df[michael_df.state != 'Georgia']\n",
    "florence_df['state'] = florence_df['username'].apply(lambda x: 'South Carolina' if x in ['SCDOTPeeDee' , 'SCDOTPiedmont','SCDOTMidlands','SCDOTLowCountry','SouthDakotaDOT'] else 'North Carolina')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "michael_df['split_text'] = michael_df['text'].str.split()\n",
    "florence_df['split_text'] = florence_df['text'].str.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "florence_df.to_csv('florence_clean.csv', index=False)\n",
    "michael_df.to_csv('michael_clean.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a list of incident\n",
    "---\n",
    "Adding status of roads for either closed, reopened, and the origin cause.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed = ['closure', 'closed', 'road work', 'Maintenance', 'clsd','avoid',\n",
    "          'stay off','block','blocked','shut down','inaccessible','closing', \n",
    "          'no vehicles', 'sealed', 'restricted', 'crash']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "reopen = ['finish','finished', 'open','opened','ended', 'fixed',\n",
    "          'reopened', 'repaired', 'cleared', 'concluded','restored']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause = ['flood', 'flooded','crash','crashed', 'accident', 'collision',\n",
    "         'collided', 'damage','damaged', 'repair', 'obstacle', 'obstacles', 'disabled',\n",
    "         'wet', 'visibility', 'hazard', 'tree', 'weather', 'slick',\n",
    "         'storm', 'maintenance','congestion','congested','weather','event']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Status\n",
    "---\n",
    "Removing any tweets that involved the reopening of a road or tweets involving no closures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_open_cause(df):\n",
    "    df['closed'] = df.text.str.extract('(?i)({0})'.format('|'.join(closed)))\n",
    "    df['open'] = df.text.str.extract('(?i)({0})'.format('|'.join(reopen)))\n",
    "    df['cause'] = df.text.str.extract('(?i)({0})'.format('|'.join(cause)))\n",
    "    return\n",
    "\n",
    "closed_open_cause(michael_df)\n",
    "closed_open_cause(florence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "michael_df = michael_df.dropna(axis=0, subset=['closed'])\n",
    "michael_df= michael_df[pd.isnull(michael_df['open'])]\n",
    "\n",
    "florence_df = florence_df.dropna(axis=0, subset=['closed'])\n",
    "florence_df= florence_df[pd.isnull(florence_df['open'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "florence_df.to_csv('florence_clean.csv', index=False)\n",
    "michael_df.to_csv('michael_clean.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
