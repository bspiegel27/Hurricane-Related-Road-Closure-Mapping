{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "pd.options.display.max_rows = 999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Scraped Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'harvey.csv' does not exist: b'harvey.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-783e0df813ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mharvey_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'harvey.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmichael_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'michael.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mflorence_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'florence.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mflorence_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflorence_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflorence_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musername\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'SouthDakotaDOT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1904\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1906\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1907\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'harvey.csv' does not exist: b'harvey.csv'"
     ]
    }
   ],
   "source": [
    "harvey_df = pd.read_csv('harvey.csv')\n",
    "michael_df = pd.read_csv('michael.csv')\n",
    "florence_df = pd.read_csv('florence.csv')\n",
    "florence_df = florence_df[florence_df.username != 'SouthDakotaDOT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "har"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Hyperlinks, Image Links, Certian Special Characters, and adding Date/time Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets (df):\n",
    "    df['text'] = df['text'].str.replace(r'pic.twitter.com.*[\\r\\n]*', '', regex=True)\n",
    "    df['text'] = df['text'].map(lambda x: re.sub('http[s]?:\\/\\/[^\\s]*',' ', x))\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    # Converting column to datetime, adding time column, making data column\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['time'] = df['date'].dt.time\n",
    "    df['date'] = df['date'].dt.date\n",
    "    #removing uneeded columns\n",
    "    df.drop(columns=['hashtags','type','geo'], inplace=True)\n",
    "    # removing certain special characters\n",
    "    df['text'] = df['text'].str.replace('.', '')\n",
    "    df['text'] = df['text'].str.replace('/', '')\n",
    "    df['text'] = df['text'].str.replace(',', '')\n",
    "    df['text'] = df['text'].str.replace(\"'\", '')\n",
    "    df['text'] = df['text'].str.replace(\";\", '')\n",
    "    df['text'] = df['text'].str.replace(\"(\", '')\n",
    "    df['text'] = df['text'].str.replace(\")\", '')\n",
    "    return\n",
    "\n",
    "    \n",
    "\n",
    "clean_tweets(harvey_df)\n",
    "clean_tweets(florence_df)\n",
    "clean_tweets(michael_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding States\n",
    "---\n",
    "Adding the state names will be essential when mapping out the locations of road closures or blockages. For Hurricane Michael and Harvey, we only observed one state, Florida and Texas, respectively. Hurrican Florence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "michael_df['state'] = michael_df['username'].map(lambda x: 'Georgia' if x in ['AlbanyGaPD','511Georgia','CityofAlbanyGA'] else 'Florida')\n",
    "michael_df = michael_df[michael_df.state != 'Georgia']\n",
    "harvey_df['state'] = 'Texas'\n",
    "florence_df['state'] = florence_df['username'].apply(lambda x: 'South Carolina' if x in ['SCDOTPeeDee' , 'SCDOTPiedmont','SCDOTMidlands','SCDOTLowCountry','SouthDakotaDOT'] else 'North Carolina')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvey_df['split_text'] = harvey_df['text'].str.split()\n",
    "michael_df['split_text'] = michael_df['text'].str.split()\n",
    "florence_df['split_text'] = florence_df['text'].str.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>state</th>\n",
       "      <th>split_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1043262364844544002</td>\n",
       "      <td>NCPublicSafety</td>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>driving in areas at risk of flooding is extrem...</td>\n",
       "      <td>22:15:05</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>[driving, in, areas, at, risk, of, flooding, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1043226122715062272</td>\n",
       "      <td>NCPublicSafety</td>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>bertie correctional institution assistant main...</td>\n",
       "      <td>19:51:04</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>[bertie, correctional, institution, assistant,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1043201830711373825</td>\n",
       "      <td>NCPublicSafety</td>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>find out more about fema disaster assistance a...</td>\n",
       "      <td>18:14:32</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>[find, out, more, about, fema, disaster, assis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1043138004339105792</td>\n",
       "      <td>NCPublicSafety</td>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>…</td>\n",
       "      <td>14:00:55</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>[…]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1043126471286501376</td>\n",
       "      <td>NCPublicSafety</td>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>if you evacuated during #hurricaneflorence it ...</td>\n",
       "      <td>13:15:05</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>[if, you, evacuated, during, #hurricaneflorenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id        username        date  \\\n",
       "0  1043262364844544002  NCPublicSafety  2018-09-21   \n",
       "1  1043226122715062272  NCPublicSafety  2018-09-21   \n",
       "2  1043201830711373825  NCPublicSafety  2018-09-21   \n",
       "3  1043138004339105792  NCPublicSafety  2018-09-21   \n",
       "4  1043126471286501376  NCPublicSafety  2018-09-21   \n",
       "\n",
       "                                                text      time  \\\n",
       "0  driving in areas at risk of flooding is extrem...  22:15:05   \n",
       "1  bertie correctional institution assistant main...  19:51:04   \n",
       "2  find out more about fema disaster assistance a...  18:14:32   \n",
       "3                                                  …  14:00:55   \n",
       "4  if you evacuated during #hurricaneflorence it ...  13:15:05   \n",
       "\n",
       "            state                                         split_text  \n",
       "0  North Carolina  [driving, in, areas, at, risk, of, flooding, i...  \n",
       "1  North Carolina  [bertie, correctional, institution, assistant,...  \n",
       "2  North Carolina  [find, out, more, about, fema, disaster, assis...  \n",
       "3  North Carolina                                                […]  \n",
       "4  North Carolina  [if, you, evacuated, during, #hurricaneflorenc...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "florence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "florence_df.to_csv('florence_clean.csv', index=False)\n",
    "michael_df.to_csv('michael_clean.csv', index=False)\n",
    "harvey_df.to_csv('harvey_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a list of incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Crash',\n",
       " 'Crashed',\n",
       " 'Accident',\n",
       " 'Collision',\n",
       " 'collided',\n",
       " 'Fatal',\n",
       " 'Tow',\n",
       " 'Break',\n",
       " 'Damage',\n",
       " 'damaged',\n",
       " 'RepairRoad work',\n",
       " 'Closure',\n",
       " 'closed',\n",
       " 'Zone',\n",
       " 'Maintenance',\n",
       " 'Schedule',\n",
       " 'Seal',\n",
       " 'Flooded',\n",
       " 'flood',\n",
       " 'broken',\n",
       " 'broke',\n",
       " 'Obstacles',\n",
       " 'Obstacle',\n",
       " 'Disabled',\n",
       " 'Stuck',\n",
       " 'Rain',\n",
       " 'Slip',\n",
       " 'Wind',\n",
       " 'Flood',\n",
       " 'Rainy',\n",
       " 'Hazard',\n",
       " 'Tree',\n",
       " 'Block',\n",
       " 'Wiper',\n",
       " 'Inches',\n",
       " 'Wet',\n",
       " 'Cold',\n",
       " 'Freeze',\n",
       " 'Hot',\n",
       " 'Visibility',\n",
       " 'Fire',\n",
       " 'Weather',\n",
       " 'Animal',\n",
       " 'Deer',\n",
       " 'Dead',\n",
       " 'Hail',\n",
       " 'Melt',\n",
       " 'Slope',\n",
       " 'Slick',\n",
       " 'Tire',\n",
       " 'Cover',\n",
       " 'Friction',\n",
       " 'Frozen',\n",
       " 'storm',\n",
       " 'traffic']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['Crash','Crashed','Accident','Collision','collided','Fatal','Tow',\n",
    "'Break','Damage','damaged','Repair''Road work','Closure','closed',\n",
    "'Zone','Maintenance','Schedule','Seal','Flooded','flood','broken','broke',\n",
    " \n",
    " 'Obstacles','Obstacle','Disabled',\n",
    "'Stuck','Rain','Slip','Wind','Flood','Rainy','Hazard','Tree','Block','Wiper',\n",
    " 'Inches','Wet',\n",
    "'Cold','Freeze','Hot','Visibility','Fire','Weather','Animal','Deer','Dead',\n",
    " 'Hail','Melt','Slope',\n",
    "'Slick','Tire','Cover','Friction','Frozen', 'storm', 'traffic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed = ['closure', 'closed', 'road work', 'Maintenance', 'clsd','avoid',\n",
    "          'stay off','block','blocked','shut down','inaccessible','closing', \n",
    "          'no vehicles', 'sealed', 'restricted', 'crash']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reopen = ['finish','finished', 'open','opened','ended', 'fixed',\n",
    "          'reopened', 'repaired', 'cleared', 'concluded','restored']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause = ['flood', 'flooded','crash','crashed', 'accident', 'collision',\n",
    "         'collided', 'damage','damaged', 'repair', 'obstacle', 'obstacles', 'disabled',\n",
    "         'wet', 'visibility', 'hazard', 'tree', 'weather', 'slick',\n",
    "         'storm', 'maintenance','congestion','congested','weather','event']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_open_cause(df):\n",
    "    df['closed'] = df.text.str.extract('(?i)({0})'.format('|'.join(closed)))\n",
    "    df['open'] = df.text.str.extract('(?i)({0})'.format('|'.join(reopen)))\n",
    "    df['cause'] = df.text.str.extract('(?i)({0})'.format('|'.join(cause)))\n",
    "    return\n",
    "\n",
    "closed_open_cause(harvey_df)\n",
    "closed_open_cause(michael_df)\n",
    "closed_open_cause(florence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvey_df = harvey_df.dropna(axis=0, subset=['closed'])\n",
    "harvey_df= harvey_df[pd.isnull(harvey_df['open'])]\n",
    "\n",
    "michael_df = michael_df.dropna(axis=0, subset=['closed'])\n",
    "michael_df= michael_df[pd.isnull(michael_df['open'])]\n",
    "\n",
    "florence_df = florence_df.dropna(axis=0, subset=['closed'])\n",
    "florence_df= florence_df[pd.isnull(florence_df['open'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "florence_df.to_csv('florence_clean.csv', index=False)\n",
    "michael_df.to_csv('michael_clean.csv', index=False)\n",
    "harvey_df.to_csv('harvey_clean.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
